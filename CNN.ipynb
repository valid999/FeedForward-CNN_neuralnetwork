{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoX0R+Msjh64Fi3Kb3kdQg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valid999/FeedForward-CNN_neuralnetwork/blob/main/CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60ZFaRHs5x49",
        "outputId": "45dca94c-0120-44ec-e48c-e0dd57f6dcf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] , Step[100/600] , Loss: 0.0802\n",
            "Epoch [1/5] , Step[200/600] , Loss: 0.1252\n",
            "Epoch [1/5] , Step[300/600] , Loss: 0.0313\n",
            "Epoch [1/5] , Step[400/600] , Loss: 0.0617\n",
            "Epoch [1/5] , Step[500/600] , Loss: 0.0135\n",
            "Epoch [1/5] , Step[600/600] , Loss: 0.0228\n",
            "Epoch [2/5] , Step[100/600] , Loss: 0.0427\n",
            "Epoch [2/5] , Step[200/600] , Loss: 0.0589\n",
            "Epoch [2/5] , Step[300/600] , Loss: 0.0990\n",
            "Epoch [2/5] , Step[400/600] , Loss: 0.1023\n",
            "Epoch [2/5] , Step[500/600] , Loss: 0.0791\n",
            "Epoch [2/5] , Step[600/600] , Loss: 0.0454\n",
            "Epoch [3/5] , Step[100/600] , Loss: 0.0667\n",
            "Epoch [3/5] , Step[200/600] , Loss: 0.0117\n",
            "Epoch [3/5] , Step[300/600] , Loss: 0.0043\n",
            "Epoch [3/5] , Step[400/600] , Loss: 0.0485\n",
            "Epoch [3/5] , Step[500/600] , Loss: 0.0062\n",
            "Epoch [3/5] , Step[600/600] , Loss: 0.0050\n",
            "Epoch [4/5] , Step[100/600] , Loss: 0.0099\n",
            "Epoch [4/5] , Step[200/600] , Loss: 0.0116\n",
            "Epoch [4/5] , Step[300/600] , Loss: 0.0292\n",
            "Epoch [4/5] , Step[400/600] , Loss: 0.0203\n",
            "Epoch [4/5] , Step[500/600] , Loss: 0.0640\n",
            "Epoch [4/5] , Step[600/600] , Loss: 0.0060\n",
            "Epoch [5/5] , Step[100/600] , Loss: 0.0162\n",
            "Epoch [5/5] , Step[200/600] , Loss: 0.0074\n",
            "Epoch [5/5] , Step[300/600] , Loss: 0.0050\n",
            "Epoch [5/5] , Step[400/600] , Loss: 0.0175\n",
            "Epoch [5/5] , Step[500/600] , Loss: 0.0881\n",
            "Epoch [5/5] , Step[600/600] , Loss: 0.0361\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Device configration\n",
        "device = torch.device('cuda' if torch.cuda.is_available()  else 'cpu')\n",
        "\n",
        "\n",
        "\n",
        "# Hyper-parameters\n",
        "\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset  = torchvision.datasets.MNIST(root = '../../data',\n",
        "                                           train  = True,\n",
        "                                           transform = transforms.ToTensor(),\n",
        "                                           download = True\n",
        "                                           )\n",
        "test_dataset = torchvision.datasets.MNIST(root = '../../data',\n",
        "                                          train = False ,\n",
        "                                          transform =  transforms.ToTensor()\n",
        "                                          )\n",
        "\n",
        "\n",
        "# Data Loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset ,\n",
        "                                           batch_size = batch_size ,\n",
        "\n",
        "                                           shuffle = True\n",
        "\n",
        "                                           )\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "\n",
        "# Convolutional neural network (two convolutional layer)\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "  def __init__(self , num_classes = 10):\n",
        "    super(ConvNet , self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(1 , 16 , kernel_size = 5 , stride = 1 , padding = 2),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 2 , stride = 2))\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(16 , 32 , kernel_size = 5 , stride = 1 , padding = 2),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2 , stride = 2))\n",
        "    self.fc = nn.Linear(7*7*32 , num_classes)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self , x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = out.reshape(out.size(0) , -1)\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = ConvNet(num_classes).to(device)\n",
        "\n",
        "# Losss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters() , lr = learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "# total the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  for i , (image , labels) in enumerate(train_loader):\n",
        "    # Move tensor to the configured device\n",
        "    images = image.to(device) # Orginal shape ([100 , 1 , 28 , 28])\n",
        "    labels = labels.to(device)\n",
        "\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs , labels)\n",
        "\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (i+1) % 100 == 0 :\n",
        "      print('Epoch [{}/{}] , Step[{}/{}] , Loss: {:.4f}'.format(epoch+1 , num_epochs , i+1 , total_step , loss.item()))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The orginal shape of the images\n",
        "\n",
        "for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.shape\n",
        "        print(images)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddiTRV_j-3iG",
        "outputId": "fa0d0462-39ce-41d8-9f15-c25c5c816aa2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_example(img , label):\n",
        "  plt.show(img.permute(1 , 2 , 0))"
      ],
      "metadata": {
        "id": "2wJ452D7DPOm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for images , labels in test_loader:\n",
        "    images = image.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    _ , predicted = torch.max(outputs.data , 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  print('Accuracy of the network on the 10000 test images : {} %'.format(100 * correct / total))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPWDiDV--5ps",
        "outputId": "1b18bb07-74be-4a6f-8317-399f33584513"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images : 10.16 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for img_batch , label_batch in train_loader:\n",
        "  print('first batch')\n",
        "  print(img_batch.shape)\n",
        "  plt.imshow(img_batch[0][0] , cmap = 'gray')\n",
        "  print(label_batch)\n",
        "  print(type(img_batch))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "-JuUH8ivFudw",
        "outputId": "1808ef3e-68c9-401b-b6c1-2e2b9087acc7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first batch\n",
            "torch.Size([100, 1, 28, 28])\n",
            "tensor([7, 6, 4, 0, 1, 7, 7, 4, 7, 7, 0, 4, 3, 3, 5, 3, 3, 9, 5, 4, 7, 3, 5, 0,\n",
            "        9, 5, 0, 3, 1, 2, 6, 0, 9, 0, 3, 8, 2, 9, 4, 0, 9, 4, 9, 4, 6, 1, 0, 8,\n",
            "        7, 5, 6, 3, 6, 1, 3, 8, 2, 9, 0, 2, 2, 4, 5, 6, 4, 7, 1, 6, 9, 7, 7, 5,\n",
            "        7, 1, 1, 7, 8, 5, 3, 8, 2, 8, 2, 5, 8, 9, 1, 7, 0, 3, 9, 1, 7, 1, 4, 4,\n",
            "        7, 1, 7, 6])\n",
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa30lEQVR4nO3df2xV9f3H8dcF6QW1vaWU9vZKgYIKiwhGBrVBEEND6RIiwhJQ/4DN4GDFDJnTlQjIfqSDJY64ISz7Q+YiykgEIsnItNgSZ8FQYYxsNrTpBFJalIR7oUip9PP9o9n9cqX8OJd7++69PB/JSei959P79njl6WlPT33OOScAAHpZP+sBAAC3JwIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBM3GE9wLd1dXWppaVFmZmZ8vl81uMAADxyzuncuXMKhULq1+/a5zl9LkAtLS0qLCy0HgMAcItOnDihYcOGXfP5PvcluMzMTOsRAAAJcKO/z5MWoI0bN2rkyJEaOHCgiouL9emnn97UOr7sBgDp4UZ/nyclQNu2bdOKFSu0Zs0affbZZ5owYYLKysp0+vTpZLwcACAVuSSYPHmyq6ioiH58+fJlFwqFXFVV1Q3XhsNhJ4mNjY2NLcW3cDh83b/vE34GdOnSJdXX16u0tDT6WL9+/VRaWqq6urqr9u/o6FAkEonZAADpL+EB+uqrr3T58mXl5+fHPJ6fn6/W1tar9q+qqlIgEIhuXAEHALcH86vgKisrFQ6Ho9uJEyesRwIA9IKE/xxQbm6u+vfvr7a2tpjH29raFAwGr9rf7/fL7/cnegwAQB+X8DOgjIwMTZw4UdXV1dHHurq6VF1drZKSkkS/HAAgRSXlTggrVqzQwoUL9d3vfleTJ0/Whg0b1N7erh/84AfJeDkAQApKSoDmz5+vL7/8UqtXr1Zra6seeugh7dmz56oLEwAAty+fc85ZD3GlSCSiQCBgPQYA4BaFw2FlZWVd83nzq+AAALcnAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgIuEBevXVV+Xz+WK2sWPHJvplAAAp7o5kfNIHHnhAH3744f+/yB1JeRkAQApLShnuuOMOBYPBZHxqAECaSMr3gI4dO6ZQKKRRo0bpmWee0fHjx6+5b0dHhyKRSMwGAEh/CQ9QcXGxtmzZoj179mjTpk1qbm7W1KlTde7cuR73r6qqUiAQiG6FhYWJHgkA0Af5nHMumS9w9uxZjRgxQq+99pqeffbZq57v6OhQR0dH9ONIJEKEACANhMNhZWVlXfP5pF8dkJ2drfvvv1+NjY09Pu/3++X3+5M9BgCgj0n6zwGdP39eTU1NKigoSPZLAQBSSMID9OKLL6q2tlb//e9/9cknn+jJJ59U//799dRTTyX6pQAAKSzhX4I7efKknnrqKZ05c0ZDhw7Vo48+qv3792vo0KGJfikAQApL+kUIXkUiEQUCAesxAAC36EYXIXAvOACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADARNJ/IR2A1FNeXu55zXvvved5zfnz5z2v4c766YMzIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJjgbtjo8+655x7PaxYvXhzXa+3evdvzmkceecTzmvz8fM9rHnroIc9rxo8f73mNJOXk5Hhek5GR4XlNOBz2vAbpgzMgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyNFr4rnxp3x3CB08ODBntdI0qpVq+Ja55XP5/O8xjmXhEkSp6ury/OadevWJWESpArOgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEz7Xx+5wGIlEFAgErMfATRg5cqTnNfX19Z7XZGdne17T1505c8bzmn/961+e10yfPt3zmnj985//9Lzm4YcfTsIk6CvC4bCysrKu+TxnQAAAEwQIAGDCc4D27dun2bNnKxQKyefzaefOnTHPO+e0evVqFRQUaNCgQSotLdWxY8cSNS8AIE14DlB7e7smTJigjRs39vj8+vXr9frrr2vz5s06cOCA7rrrLpWVlenixYu3PCwAIH14/o2o5eXlKi8v7/E555w2bNigV155RU888YQk6a233lJ+fr527typBQsW3Nq0AIC0kdDvATU3N6u1tVWlpaXRxwKBgIqLi1VXV9fjmo6ODkUikZgNAJD+Ehqg1tZWSVJ+fn7M4/n5+dHnvq2qqkqBQCC6FRYWJnIkAEAfZX4VXGVlpcLhcHQ7ceKE9UgAgF6Q0AAFg0FJUltbW8zjbW1t0ee+ze/3KysrK2YDAKS/hAaoqKhIwWBQ1dXV0ccikYgOHDigkpKSRL4UACDFeb4K7vz582psbIx+3NzcrMOHDysnJ0fDhw/X8uXL9atf/Ur33XefioqKtGrVKoVCIc2ZMyeRcwMAUpznAB08eFCPP/549OMVK1ZIkhYuXKgtW7bopZdeUnt7u5577jmdPXtWjz76qPbs2aOBAwcmbmoAQMrjZqSI29///nfPa2bMmJGESa7285//PK5127dv97wmnh8d+OabbzyvmT17tuc1b731luc18Vq5cqXnNevWrUvCJOgruBkpAKBPIkAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAnPv44B6WfatGlxrXvsscc8r+nq6vK85tChQ57XbNiwwfMaSers7IxrXW+YO3eu9QjXFc9dwXF74wwIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADDBzUih9vb2uNZ99tlnntccPXrU85rFixd7XtPX5efne14zderUJEzSs5aWFs9r/vKXvyRhEqQzzoAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABPcjBSqr6+Pa108N8f85ptv4nqtdPPrX//a85ohQ4YkYZKevfHGG57XnD9/PgmTIJ1xBgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmOBmpIgbNxaNX1ZWVq+8zpkzZ+Ja94c//CHBkwBX4wwIAGCCAAEATHgO0L59+zR79myFQiH5fD7t3Lkz5vlFixbJ5/PFbLNmzUrUvACANOE5QO3t7ZowYYI2btx4zX1mzZqlU6dORbd33nnnloYEAKQfzxchlJeXq7y8/Lr7+P1+BYPBuIcCAKS/pHwPqKamRnl5eRozZoyWLl163StxOjo6FIlEYjYAQPpLeIBmzZqlt956S9XV1Vq3bp1qa2tVXl6uy5cv97h/VVWVAoFAdCssLEz0SACAPijhPwe0YMGC6J8ffPBBjR8/XqNHj1ZNTY1mzJhx1f6VlZVasWJF9ONIJEKEAOA2kPTLsEeNGqXc3Fw1Njb2+Lzf71dWVlbMBgBIf0kP0MmTJ3XmzBkVFBQk+6UAACnE85fgzp8/H3M209zcrMOHDysnJ0c5OTlau3at5s2bp2AwqKamJr300ku69957VVZWltDBAQCpzXOADh48qMcffzz68f++f7Nw4UJt2rRJR44c0Z///GedPXtWoVBIM2fO1C9/+Uv5/f7ETQ0ASHk+55yzHuJKkUhEgUDAegzgpo0ePdrzmmPHjnleE89/qps3b/a8RpIqKiriWgdcKRwOX/f7+twLDgBgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYS/iu5gdvNH//4R89rfD5fEia52ieffNIrrwPEgzMgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAENyMFrhAIBDyvKSoq8rzGOed5zaFDhzyv2b59u+c1QG/hDAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHNSIErPP30057XjBw5MvGD9GDbtm2e11y6dCkJkwCJwRkQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGDC55xz1kNcKRKJKBAIWI+B29Tnn3/uec19993nec3Ro0c9rykpKfG85sKFC57XAIkSDoeVlZV1zec5AwIAmCBAAAATngJUVVWlSZMmKTMzU3l5eZozZ44aGhpi9rl48aIqKio0ZMgQ3X333Zo3b57a2toSOjQAIPV5ClBtba0qKiq0f/9+ffDBB+rs7NTMmTPV3t4e3eeFF17Q+++/r+3bt6u2tlYtLS2aO3duwgcHAKS2W7oI4csvv1ReXp5qa2s1bdo0hcNhDR06VFu3btX3v/99Sd3f1P3Od76juro6PfLIIzf8nFyEAEtchAAkTlIvQgiHw5KknJwcSVJ9fb06OztVWloa3Wfs2LEaPny46urqevwcHR0dikQiMRsAIP3FHaCuri4tX75cU6ZM0bhx4yRJra2tysjIUHZ2dsy++fn5am1t7fHzVFVVKRAIRLfCwsJ4RwIApJC4A1RRUaGjR4/q3XffvaUBKisrFQ6Ho9uJEydu6fMBAFLDHfEsWrZsmXbv3q19+/Zp2LBh0ceDwaAuXbqks2fPxpwFtbW1KRgM9vi5/H6//H5/PGMAAFKYpzMg55yWLVumHTt2aO/evSoqKop5fuLEiRowYICqq6ujjzU0NOj48eNxfQMVAJC+PJ0BVVRUaOvWrdq1a5cyMzOj39cJBAIaNGiQAoGAnn32Wa1YsUI5OTnKysrS888/r5KSkpu6Ag4AcPvwFKBNmzZJkqZPnx7z+JtvvqlFixZJkn73u9+pX79+mjdvnjo6OlRWVqY33ngjIcMCANIHNyNFWpo5c2Zc63bv3u15Tf/+/T2vWbJkiec1f/rTnzyvASxxM1IAQJ9EgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAE3H9RlSgr1u5cmVc6+K5s3VHR4fnNYcOHfK8Bkg3nAEBAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GSnS0sSJE3vttX70ox95XnPw4MEkTAKkFs6AAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAAT3IwUfd66des8rxk0aFASJunZyZMne+21gHTCGRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYMLnnHPWQ1wpEokoEAhYj4EkGTx4sOc1X3zxhec1d911l+c1ktTU1OR5zUMPPeR5zYULFzyvAVJNOBxWVlbWNZ/nDAgAYIIAAQBMeApQVVWVJk2apMzMTOXl5WnOnDlqaGiI2Wf69Ony+Xwx25IlSxI6NAAg9XkKUG1trSoqKrR//3598MEH6uzs1MyZM9Xe3h6z3+LFi3Xq1Knotn79+oQODQBIfZ5+I+qePXtiPt6yZYvy8vJUX1+vadOmRR+/8847FQwGEzMhACAt3dL3gMLhsCQpJycn5vG3335bubm5GjdunCorK697xU9HR4cikUjMBgBIf57OgK7U1dWl5cuXa8qUKRo3blz08aefflojRoxQKBTSkSNH9PLLL6uhoUHvvfdej5+nqqpKa9eujXcMAECKivvngJYuXaq//e1v+vjjjzVs2LBr7rd3717NmDFDjY2NGj169FXPd3R0qKOjI/pxJBJRYWFhPCMhBfBzQN34OSDcDm70c0BxnQEtW7ZMu3fv1r59+64bH0kqLi6WpGsGyO/3y+/3xzMGACCFeQqQc07PP/+8duzYoZqaGhUVFd1wzeHDhyVJBQUFcQ0IAEhPngJUUVGhrVu3ateuXcrMzFRra6skKRAIaNCgQWpqatLWrVv1ve99T0OGDNGRI0f0wgsvaNq0aRo/fnxS/gEAAKnJU4A2bdokqfuHTa/05ptvatGiRcrIyNCHH36oDRs2qL29XYWFhZo3b55eeeWVhA0MAEgPnr8Edz2FhYWqra29pYEAALeHuC/DBuLR2dnpec3Bgwc9r3nsscc8r5GkH/7wh57XcEUbEB9uRgoAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmIj7V3InSyQSUSAQsB4DAHCLbvQruTkDAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYKLPBaiP3ZoOABCnG/193ucCdO7cOesRAAAJcKO/z/vc3bC7urrU0tKizMxM+Xy+mOcikYgKCwt14sSJ695hNd1xHLpxHLpxHLpxHLr1hePgnNO5c+cUCoXUr9+1z3Pu6MWZbkq/fv00bNiw6+6TlZV1W7/B/ofj0I3j0I3j0I3j0M36ONzMr9Xpc1+CAwDcHggQAMBESgXI7/drzZo18vv91qOY4jh04zh04zh04zh0S6Xj0OcuQgAA3B5S6gwIAJA+CBAAwAQBAgCYIEAAABMpE6CNGzdq5MiRGjhwoIqLi/Xpp59aj9TrXn31Vfl8vpht7Nix1mMl3b59+zR79myFQiH5fD7t3Lkz5nnnnFavXq2CggINGjRIpaWlOnbsmM2wSXSj47Bo0aKr3h+zZs2yGTZJqqqqNGnSJGVmZiovL09z5sxRQ0NDzD4XL15URUWFhgwZorvvvlvz5s1TW1ub0cTJcTPHYfr06Ve9H5YsWWI0cc9SIkDbtm3TihUrtGbNGn322WeaMGGCysrKdPr0aevRet0DDzygU6dORbePP/7YeqSka29v14QJE7Rx48Yen1+/fr1ef/11bd68WQcOHNBdd92lsrIyXbx4sZcnTa4bHQdJmjVrVsz745133unFCZOvtrZWFRUV2r9/vz744AN1dnZq5syZam9vj+7zwgsv6P3339f27dtVW1urlpYWzZ0713DqxLuZ4yBJixcvjnk/rF+/3mjia3ApYPLkya6ioiL68eXLl10oFHJVVVWGU/W+NWvWuAkTJliPYUqS27FjR/Tjrq4uFwwG3W9/+9voY2fPnnV+v9+98847BhP2jm8fB+ecW7hwoXviiSdM5rFy+vRpJ8nV1tY657r/3Q8YMMBt3749us9//vMfJ8nV1dVZjZl03z4Ozjn32GOPuZ/85Cd2Q92EPn8GdOnSJdXX16u0tDT6WL9+/VRaWqq6ujrDyWwcO3ZMoVBIo0aN0jPPPKPjx49bj2SqublZra2tMe+PQCCg4uLi2/L9UVNTo7y8PI0ZM0ZLly7VmTNnrEdKqnA4LEnKycmRJNXX16uzszPm/TB27FgNHz48rd8P3z4O//P2228rNzdX48aNU2VlpS5cuGAx3jX1uZuRfttXX32ly5cvKz8/P+bx/Px8ff7550ZT2SguLtaWLVs0ZswYnTp1SmvXrtXUqVN19OhRZWZmWo9norW1VZJ6fH/877nbxaxZszR37lwVFRWpqalJK1euVHl5uerq6tS/f3/r8RKuq6tLy5cv15QpUzRu3DhJ3e+HjIwMZWdnx+ybzu+Hno6DJD399NMaMWKEQqGQjhw5opdfflkNDQ167733DKeN1ecDhP9XXl4e/fP48eNVXFysESNG6K9//aueffZZw8nQFyxYsCD65wcffFDjx4/X6NGjVVNToxkzZhhOlhwVFRU6evTobfF90Ou51nF47rnnon9+8MEHVVBQoBkzZqipqUmjR4/u7TF71Oe/BJebm6v+/ftfdRVLW1ubgsGg0VR9Q3Z2tu6//341NjZaj2Lmf+8B3h9XGzVqlHJzc9Py/bFs2TLt3r1bH330UcyvbwkGg7p06ZLOnj0bs3+6vh+udRx6UlxcLEl96v3Q5wOUkZGhiRMnqrq6OvpYV1eXqqurVVJSYjiZvfPnz6upqUkFBQXWo5gpKipSMBiMeX9EIhEdOHDgtn9/nDx5UmfOnEmr94dzTsuWLdOOHTu0d+9eFRUVxTw/ceJEDRgwIOb90NDQoOPHj6fV++FGx6Enhw8flqS+9X6wvgriZrz77rvO7/e7LVu2uH//+9/uueeec9nZ2a61tdV6tF7105/+1NXU1Ljm5mb3j3/8w5WWlrrc3Fx3+vRp69GS6ty5c+7QoUPu0KFDTpJ77bXX3KFDh9wXX3zhnHPuN7/5jcvOzna7du1yR44ccU888YQrKipyX3/9tfHkiXW943Du3Dn34osvurq6Otfc3Ow+/PBD9/DDD7v77rvPXbx40Xr0hFm6dKkLBAKupqbGnTp1KrpduHAhus+SJUvc8OHD3d69e93BgwddSUmJKykpMZw68W50HBobG90vfvELd/DgQdfc3Ox27drlRo0a5aZNm2Y8eayUCJBzzv3+9793w4cPdxkZGW7y5Mlu//791iP1uvnz57uCggKXkZHh7rnnHjd//nzX2NhoPVbSffTRR07SVdvChQudc92XYq9atcrl5+c7v9/vZsyY4RoaGmyHToLrHYcLFy64mTNnuqFDh7oBAwa4ESNGuMWLF6fd/6T19M8vyb355pvRfb7++mv34x//2A0ePNjdeeed7sknn3SnTp2yGzoJbnQcjh8/7qZNm+ZycnKc3+939957r/vZz37mwuGw7eDfwq9jAACY6PPfAwIApCcCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwMT/AbQtynMI0NvhAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict() , 'model.ckpt')"
      ],
      "metadata": {
        "id": "rxWbGriIIfaf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def predict_image(img, model):\n",
        "    xb = img.unsqueeze(0)\n",
        "    yb = model(xb)\n",
        "    _, preds = torch.max(yb, dim=1)\n",
        "    return preds[0].item()"
      ],
      "metadata": {
        "id": "nB3rykfBIoiO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for img, label in test_loader:\n",
        "\n",
        "#   plt.imshow(img[0][0], cmap='gray')\n",
        "#   print('Label:', label, ', Predicted:', predict_image(img, model))"
      ],
      "metadata": {
        "id": "SyW5E_0cLFVb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}